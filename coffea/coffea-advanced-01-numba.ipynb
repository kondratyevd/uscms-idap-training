{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coffea Processors\n",
    "This is a rendered copy of [processor.ipynb](https://github.com/CoffeaTeam/coffea/blob/master/binder/processor.ipynb). You can optionally run it interactively on [binder at this link](https://mybinder.org/v2/gh/coffeateam/coffea/master?filepath=binder%2Fprocessor.ipynb)\n",
    "\n",
    "Coffea relies mainly on [uproot](https://github.com/scikit-hep/uproot) to provide access to ROOT files for analysis.\n",
    "As a usual analysis will involve processing tens to thousands of files, totalling gigabytes to terabytes of data, there is a certain amount of work to be done to build a parallelized framework to process the data in a reasonable amount of time. Of course, one can work directly within uproot to achieve this, as we'll show in the beginning, but coffea provides the `coffea.processor` module, which allows users to worry just about the actual analysis code and not about how to implement efficient parallelization, assuming that the parallization is a trivial map-reduce operation (e.g. filling histograms and adding them together). The module provides the following key features:\n",
    "\n",
    " * A `ProcessorABC` abstract base class that can be derived from to implement the analysis code;\n",
    " * A [NanoEvents](https://coffeateam.github.io/coffea/notebooks/nanoevents.html) interface to the arrays being read from the TTree as inputs;\n",
    " * A generic `accumulate()` utility to reduce the outputs to a single result, as showin in the accumulators notebook tutorial; and\n",
    " * A set of parallel executors to access multicore processing or distributed computing systems such as [Dask](https://distributed.dask.org/en/latest/), [Parsl](http://parsl-project.org/), [Spark](https://spark.apache.org/), [WorkQueue](https://cctools.readthedocs.io/en/latest/work_queue/), and others.\n",
    "\n",
    "Let's start by writing a simple processor class that reads some CMS open data and plots a dimuon mass spectrum.\n",
    "We'll start by copying the [ProcessorABC](https://coffeateam.github.io/coffea/api/coffea.processor.ProcessorABC.html#coffea.processor.ProcessorABC) skeleton and filling in some details:\n",
    "\n",
    " * Remove `flag`, as we won't use it\n",
    " * Adding a new histogram for $m_{\\mu \\mu}$\n",
    " * Building a [Candidate](https://coffeateam.github.io/coffea/api/coffea.nanoevents.methods.candidate.PtEtaPhiMCandidate.html#coffea.nanoevents.methods.candidate.PtEtaPhiMCandidate) record for muons, since we will read it with `BaseSchema` interpretation (the files used here could be read with `NanoAODSchema` but we want to show how to build vector objects from other TTree formats) \n",
    " * Calculating the dimuon invariant mass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import hist\n",
    "import dask\n",
    "import awkward as ak\n",
    "import hist.dask as hda\n",
    "import dask_awkward as dak\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from coffea import processor\n",
    "from coffea.nanoevents import NanoEventsFactory, BaseSchema\n",
    "from coffea.nanoevents.methods import candidate\n",
    "from coffea.dataset_tools import (\n",
    "    apply_to_fileset,\n",
    "    max_chunks,\n",
    "    preprocess,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "\n",
    "client = Client()\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fileset = {\n",
    "    'DoubleMuon': {\n",
    "        \"files\": {\n",
    "            '/work/projects/hats2024/data/Run2012B_DoubleMuParked.root': \"Events\",\n",
    "            #'root://eospublic.cern.ch//eos/root-eos/cms_opendata_2012_nanoaod/Run2012B_DoubleMuParked.root': \"Events\",\n",
    "            '/work/projects/hats2024/data/Run2012C_DoubleMuParked.root': \"Events\",\n",
    "            #'root://eospublic.cern.ch//eos/root-eos/cms_opendata_2012_nanoaod/Run2012C_DoubleMuParked.root': \"Events\"\n",
    "        },\n",
    "        \"metadata\": {\n",
    "            \"is_mc\": False,\n",
    "        },\n",
    "    },\n",
    "    'ZZ to 4mu': {\n",
    "        \"files\": {\n",
    "            '/work/projects/hats2024/data/ZZTo4mu.root': \"Events\"\n",
    "            #'root://eospublic.cern.ch//eos/root-eos/cms_opendata_2012_nanoaod/ZZTo4mu.root': \"Events\"\n",
    "        },\n",
    "        \"metadata\": {\n",
    "            \"is_mc\": True,\n",
    "        },\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "dataset_runnable, dataset_updated = preprocess(\n",
    "    fileset,\n",
    "    align_clusters=False,\n",
    "    step_size=100_000,\n",
    "    files_per_batch=1,\n",
    "    skip_bad_files=True,\n",
    "    save_form=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting fancy\n",
    "Let's flesh out the processor from the coffea-04-processing notebook into a 4-muon analysis, searching for diboson events:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numba\n",
    "\n",
    "\n",
    "@numba.njit\n",
    "def find_4lep_kernel(events_leptons, builder):\n",
    "    \"\"\"Search for valid 4-lepton combinations from an array of events * leptons {charge, ...}\n",
    "    \n",
    "    A valid candidate has two pairs of leptons that each have balanced charge\n",
    "    Outputs an array of events * candidates {indices 0..3} corresponding to all valid\n",
    "    permutations of all valid combinations of unique leptons in each event\n",
    "    (omitting permutations of the pairs)\n",
    "    \"\"\"\n",
    "    for leptons in events_leptons:\n",
    "        builder.begin_list()\n",
    "        nlep = len(leptons)\n",
    "        for i0 in range(nlep):\n",
    "            for i1 in range(i0 + 1, nlep):\n",
    "                if leptons[i0].charge + leptons[i1].charge != 0:\n",
    "                    continue\n",
    "                for i2 in range(nlep):\n",
    "                    for i3 in range(i2 + 1, nlep):\n",
    "                        if len({i0, i1, i2, i3}) < 4:\n",
    "                            continue\n",
    "                        if leptons[i2].charge + leptons[i3].charge != 0:\n",
    "                            continue\n",
    "                        builder.begin_tuple(4)\n",
    "                        builder.index(0).integer(i0)\n",
    "                        builder.index(1).integer(i1)\n",
    "                        builder.index(2).integer(i2)\n",
    "                        builder.index(3).integer(i3)\n",
    "                        builder.end_tuple()\n",
    "        builder.end_list()\n",
    "\n",
    "    return builder\n",
    "\n",
    "\n",
    "def find_4lep(events_leptons):\n",
    "    if ak.backend(events_leptons) == \"typetracer\":\n",
    "        # here we fake the output of find_4lep_kernel since\n",
    "        # operating on length-zero data returns the wrong layout!\n",
    "        ak.typetracer.length_zero_if_typetracer(events_leptons.charge) # force touching of the necessary data\n",
    "        return ak.Array(ak.Array([[(0,0,0,0)]]).layout.to_typetracer(forget_length=True))\n",
    "    return find_4lep_kernel(events_leptons, ak.ArrayBuilder()).snapshot()\n",
    "\n",
    "\n",
    "class FancyDimuonProcessor(processor.ProcessorABC):\n",
    "    def process(self, events):\n",
    "        dataset_axis = hist.axis.StrCategory([], growth=True, name=\"dataset\", label=\"Primary dataset\")\n",
    "        mass_axis = hist.axis.Regular(300, 0, 300, name=\"mass\", label=r\"$m_{\\mu\\mu}$ [GeV]\")\n",
    "        pt_axis = hist.axis.Regular(300, 0, 300, name=\"pt\", label=r\"$p_{T,\\mu}$ [GeV]\")\n",
    "        \n",
    "        h_nMuons = hda.Hist(\n",
    "            dataset_axis,\n",
    "            hda.hist.hist.axis.IntCategory(range(6), name=\"nMuons\", label=\"Number of good muons\"),\n",
    "            storage=\"weight\", label=\"Counts\",\n",
    "        )\n",
    "        h_m4mu = hda.hist.Hist(dataset_axis, mass_axis, storage=\"weight\", label=\"Counts\")\n",
    "        h_mZ1 = hda.hist.Hist(dataset_axis, mass_axis, storage=\"weight\", label=\"Counts\")\n",
    "        h_mZ2 = hda.hist.Hist(dataset_axis, mass_axis, storage=\"weight\", label=\"Counts\")\n",
    "        h_ptZ1mu1 = hda.hist.Hist(dataset_axis, pt_axis, storage=\"weight\", label=\"Counts\")\n",
    "        h_ptZ1mu2 = hda.hist.Hist(dataset_axis, pt_axis, storage=\"weight\", label=\"Counts\")\n",
    "                \n",
    "        cutflow = defaultdict(int)\n",
    "        \n",
    "        dataset = events.metadata['dataset']\n",
    "        muons = ak.zip({\n",
    "            \"pt\": events.Muon_pt,\n",
    "            \"eta\": events.Muon_eta,\n",
    "            \"phi\": events.Muon_phi,\n",
    "            \"mass\": events.Muon_mass,\n",
    "            \"charge\": events.Muon_charge,\n",
    "            \"isolation\": events.Muon_pfRelIso03_all,\n",
    "        }, with_name=\"PtEtaPhiMCandidate\", behavior=candidate.behavior)\n",
    "        \n",
    "        # make sure they are sorted by transverse momentum\n",
    "        muons = muons[ak.argsort(muons.pt, axis=1)]\n",
    "        \n",
    "        cutflow['all events'] = ak.num(muons, axis=0)\n",
    "        \n",
    "        # impose some quality and minimum pt cuts on the muons\n",
    "        muons = muons[\n",
    "            (muons.pt > 5)\n",
    "            & (muons.isolation < 0.2)\n",
    "        ]\n",
    "        cutflow['at least 4 good muons'] += ak.sum(ak.num(muons) >= 4)\n",
    "        h_nMuons.fill(dataset=dataset, nMuons=ak.num(muons))\n",
    "        \n",
    "        # reduce first axis: skip events without enough muons\n",
    "        muons = muons[ak.num(muons) >= 4]\n",
    "        \n",
    "        # find all candidates with helper function\n",
    "        fourmuon = dak.map_partitions(find_4lep, muons)\n",
    "        fourmuon = [muons[fourmuon[idx]] for idx in \"0123\"]\n",
    "\n",
    "        fourmuon = ak.zip({\n",
    "            \"z1\": ak.zip({\n",
    "                \"lep1\": fourmuon[0],\n",
    "                \"lep2\": fourmuon[1],\n",
    "                \"p4\": fourmuon[0] + fourmuon[1],\n",
    "            }),\n",
    "            \"z2\": ak.zip({\n",
    "                \"lep1\": fourmuon[2],\n",
    "                \"lep2\": fourmuon[3],\n",
    "                \"p4\": fourmuon[2] + fourmuon[3],\n",
    "            }),\n",
    "        })\n",
    "\n",
    "        cutflow['at least one candidate'] += ak.sum(ak.num(fourmuon) > 0)\n",
    "\n",
    "        # require minimum dimuon mass\n",
    "        fourmuon = fourmuon[(fourmuon.z1.p4.mass > 60.) & (fourmuon.z2.p4.mass > 20.)]\n",
    "        cutflow['minimum dimuon mass'] += ak.sum(ak.num(fourmuon) > 0)\n",
    "\n",
    "        # choose permutation with z1 mass closest to nominal Z boson mass\n",
    "        bestz1 = ak.singletons(ak.argmin(abs(fourmuon.z1.p4.mass - 91.1876), axis=1))\n",
    "        fourmuon = ak.flatten(fourmuon[bestz1])\n",
    "\n",
    "        h_m4mu.fill(\n",
    "            dataset=dataset,\n",
    "            mass=(fourmuon.z1.p4 + fourmuon.z2.p4).mass,\n",
    "        )\n",
    "        h_mZ1.fill(\n",
    "            dataset=dataset, \n",
    "            mass=fourmuon.z1.p4.mass,\n",
    "        )\n",
    "        h_mZ2.fill(\n",
    "            dataset=dataset, \n",
    "            mass=fourmuon.z2.p4.mass,\n",
    "        )\n",
    "        h_ptZ1mu1.fill(\n",
    "            dataset=dataset,\n",
    "            pt=fourmuon.z1.lep1.pt,\n",
    "        )\n",
    "        h_ptZ1mu2.fill(\n",
    "            dataset=dataset,\n",
    "            pt=fourmuon.z1.lep2.pt,\n",
    "        )\n",
    "        return {\n",
    "            'nMuons': h_nMuons,\n",
    "            'mass': h_m4mu,\n",
    "            'mass_z1': h_mZ1,\n",
    "            'mass_z2': h_mZ2,\n",
    "            'pt_z1_mu1': h_ptZ1mu1,\n",
    "            'pt_z1_mu2': h_ptZ1mu2,\n",
    "            'cutflow': {dataset: cutflow},\n",
    "        }\n",
    "\n",
    "    def postprocess(self, accumulator):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "tstart = time.time()\n",
    "\n",
    "to_compute = apply_to_fileset(\n",
    "                FancyDimuonProcessor(),\n",
    "                max_chunks(dataset_runnable, 300),\n",
    "                schemaclass=BaseSchema,\n",
    "            )\n",
    "(out,) = dask.compute(to_compute)\n",
    "print(out)\n",
    "\n",
    "elapsed = time.time() - tstart\n",
    "print(elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nevt = out['ZZ to 4mu']['cutflow']['ZZ to 4mu']['all events'] + out['DoubleMuon']['cutflow']['DoubleMuon']['all events']\n",
    "print(\"Events/s:\", (nevt / elapsed).compute())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What follows is just us looking at the output, you can execute it if you wish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# scale ZZ simulation to expected yield\n",
    "lumi = 11.6  # 1/fb\n",
    "zzxs = 7200 * 0.0336**2  # approximate 8 TeV ZZ(4mu)\n",
    "nzz = out['ZZ to 4mu']['cutflow']['ZZ to 4mu']['all events']\n",
    "\n",
    "scaled = {}\n",
    "for (name1, h1), (nam2, h2) in zip(out['ZZ to 4mu'].items(), out['DoubleMuon'].items()):\n",
    "    if isinstance(h1, hist.Hist) and isinstance(h2, hist.Hist):\n",
    "        scaled[name1] = h1.copy() + h2.copy()\n",
    "        scaled[name1].view()[0, :] *= lumi * zzxs / nzz.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "scaled['nMuons'].plot1d(ax=ax, overlay='dataset')\n",
    "ax.set_yscale('log')\n",
    "ax.set_ylim(1, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "scaled['mass'][:, ::hist.rebin(4)].plot1d(ax=ax, overlay='dataset');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "scaled['mass_z1'].plot1d(ax=ax, overlay='dataset');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "scaled['mass_z2'].plot1d(ax=ax, overlay='dataset')\n",
    "ax.set_xlim(2, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "scaled['pt_z1_mu1'].plot1d(ax=ax, overlay='dataset');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "scaled['pt_z1_mu2'].plot1d(ax=ax, overlay='dataset');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HATS 2024",
   "language": "python",
   "name": "hats2024"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
